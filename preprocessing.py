# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1asjp1djLv264aHye56XBqT3Hrg7e-A0Z

# Import Packages
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import f_oneway
import statsmodels.api as sm
import itertools
from scipy.stats import chi2_contingency
from sklearn.feature_selection import mutual_info_regression, mutual_info_classif
from sklearn.preprocessing import StandardScaler

"""# Load Data"""

df = pd.read_csv('Rakamin Bootcamp - Dataset - Promotion Dataset.csv')
df.head()

"""# Basic Analysis

## Numeric Analysis
"""

df.info()

df.describe()

df['Current_Position_Level'].describe()

df[df["Promotion_Eligible"] == 1].describe()

df.loc[df["Promotion_Eligible"] == 1, "Current_Position_Level"].describe()

df[df["Promotion_Eligible"] == 0].describe()

df.loc[df["Promotion_Eligible"] == 0, "Current_Position_Level"].describe()



"""## Garbage Data Detection"""

# Fungsi untuk melakukan pengecekan jumlah baris yang nilai suatu kolom nya null
def null_checker(col_name):
  print ('Jumlah baris \'' + col_name + '\' yang null:', df[col_name].isna().sum())

# Fungsi untuk melakukan pengecekan jumlah baris yang nilai suatu kolom nya negative
def negative_checker(col_name):
  print ('Jumlah baris \'' + col_name + '\' yang bernilai negatif:', (df[col_name] < 0).sum())

"""### 'Employee_ID' Value Check"""

null_checker('Employee_ID')

# Cek apakah kolom ini ada yang duplikat
print ('Baris \'Employee_ID\' yang duplikat:\n\n', df[df['Employee_ID'].duplicated()])

"""### 'Age' Value Check"""

null_checker('Age')

negative_checker('Age')

"""### 'Years_at_Company' Value Check"""

null_checker('Years_at_Company')

negative_checker('Years_at_Company')

"""### 'Performance_Score' Value Check"""

null_checker('Performance_Score')

negative_checker('Performance_Score')

"""### 'Leadership_Score' Value Check"""

null_checker('Leadership_Score')

negative_checker('Leadership_Score')

"""### 'Training_Hours' Value Check"""

null_checker('Training_Hours')

negative_checker('Training_Hours')

"""### 'Projects_Handled' Value Check"""

null_checker('Projects_Handled')

negative_checker('Projects_Handled')

"""### 'Peer_Review_Score' Value Check"""

null_checker('Peer_Review_Score')

negative_checker('Peer_Review_Score')

"""### 'Current_Position_Level' Value Check"""

null_checker('Current_Position_Level')

"""### 'Promotion_Eligible' Value Check"""

null_checker('Promotion_Eligible')

negative_checker('Promotion_Eligible')

"""## Conclusion

*   Missing/Null Values:
    *   Semua kolom kecuali 'Employee_ID'
*   Negative Values:
    *   'Age', 'Years_at_Company', 'Training_Hours'
*   Questionable Values:
    *   Selain nilai negatif, 'Age' ada yang hanya bernilai 10, dan sampai 200
    *   Selain nilai negatif, 'Years_at_Company' bernilai sampai 100
    *   Selain nilai negatif, 'Training_Hours' bernilai sampai 5000
    *   Ada beberapa karyawan yang nilai 'Age' nya hampir sama dengan nilai 'Years_at_Company'
        *   Contoh: EMP0001 berumur 24 tahun, namun sudah 21 tahun di perusahaan tersebut.

# Exploratory Data Analysis

## Univariate Analysis
"""

features =['Age', 'Years_at_Company',
       'Leadership_Score', 'Training_Hours', 'Projects_Handled',
       'Peer_Review_Score']
for i in range(0, len(features)):
    plt.subplot (2, 3, i+1)
    sns.kdeplot(x=df[features[i]], color='blue')
    plt.xlabel(features[i])
    plt.tight_layout()



"""### 'Age' Analysis"""

counts = (
    df['Age']
    .value_counts()
    .sort_index()
)
xmin = int(counts.index.min())
xmax = int(counts.index.max())

plt.figure(figsize=(12, 8))
plt.plot(counts.index, counts.values)

plt.xticks(range(xmin - xmin % 10, xmax + 10, 10))
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Distribution of Age')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 'Years_at_Company' Analysis"""

counts = (
    df['Years_at_Company']
    .value_counts()
    .sort_index()
)
xmin = int(counts.index.min())
xmax = int(counts.index.max())

plt.figure(figsize=(12, 8))
plt.plot(counts.index, counts.values)

plt.xticks(range(xmin - xmin % 10, xmax + 10, 10))
plt.xlabel('Years_at_Company')
plt.ylabel('Frequency')
plt.title('Distribution of Years_at_Company')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 'Performance_Score' Analysis"""

plt.figure(figsize=(12, 8))
plt.title('Distribution of Performance_Score')
plt.ylabel('Frequency')
sns.countplot(data=df, x="Performance_Score")
plt.show()

"""### 'Leadership_Score' Analysis"""

counts = (
    df['Leadership_Score']
    .value_counts()
    .sort_index()
)
xmin = int(counts.index.min())
xmax = int(counts.index.max())

plt.figure(figsize=(12, 8))
plt.plot(counts.index, counts.values)

plt.xticks(range(xmin - xmin % 5, xmax + 5, 5))
plt.xlabel('Leadership_Score')
plt.ylabel('Frequency')
plt.title('Distribution of Leadership_Score')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 'Training_Hours' Analysis"""

counts = (
    df['Training_Hours']
    .value_counts()
    .sort_index()
)
xmin = int(counts.index.min())
xmax = int(counts.index.max())

plt.figure(figsize=(12, 8))
plt.plot(counts.index, counts.values)

# plt.xticks(range(xmin - xmin % 5, xmax + 5, 5))
plt.xlabel('Training_Hours')
plt.ylabel('Frequency')
plt.title('Distribution of Training_Hours')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 'Projects_Handled' Analysis"""

counts = (
    df['Projects_Handled']
    .value_counts()
    .sort_index()
)
xmin = int(counts.index.min())
xmax = int(counts.index.max())

plt.figure(figsize=(12, 8))
plt.plot(counts.index, counts.values)

plt.xticks(range(xmin - xmin % 2, xmax + 2, 2))
plt.xlabel('Projects_Handled')
plt.ylabel('Frequency')
plt.title('Distribution of Projects_Handled')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 'Peer_Review_Score' Analysis"""

counts = (
    df['Peer_Review_Score']
    .value_counts()
    .sort_index()
)
xmin = int(counts.index.min())
xmax = int(counts.index.max())

plt.figure(figsize=(12, 8))
plt.plot(counts.index, counts.values)

plt.xticks(range(xmin - xmin % 5, xmax + 5, 5))
plt.xlabel('Peer_Review_Score')
plt.ylabel('Frequency')
plt.title('Distribution of Peer_Review_Score')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### 'Current_Position_Level' Analysis"""

order = ["Junior", "Mid", "Senior", "Lead"]

plt.figure(figsize=(12, 8))
ax = sns.countplot(data=df, x="Current_Position_Level", order=order)
for container in ax.containers:
    ax.bar_label(container)

plt.title('Distribution of Current_Position_Level')
plt.ylabel('Frequency')
plt.show()

"""### 'Promotion_Eligible' Analysis"""

df["Promotion_Eligible"] = df["Promotion_Eligible"].astype("Int64")

mapping = {
    '0.0': "No",
    '1.0': "Yes",
    '0': "No",
    '1': "Yes",
    0: "No",
    1: "Yes",
}

plt.figure(figsize=(12, 8))
ax = sns.countplot(data=df, x="Promotion_Eligible")
for container in ax.containers:
    ax.bar_label(container)

ax.set_xticklabels([mapping[tick.get_text()] for tick in ax.get_xticklabels()])
plt.title('Distribution of Promotion_Eligible')
plt.ylabel('Frequency')
plt.show()

"""## Bivariate Analysis"""

# Drop rows where the target is NaN
df_clean = df.dropna(subset=["Promotion_Eligible", "Age", "Years_at_Company", "Performance_Score", "Leadership_Score", "Performance_Score", "Training_Hours", "Projects_Handled", "Peer_Review_Score",
                             "Current_Position_Level"])

# Update X and y
X = df_clean.drop(columns=["Promotion_Eligible", "Employee_ID"])
X_ohe = pd.get_dummies(X, drop_first=True)

y = df_clean["Promotion_Eligible"]

mi = mutual_info_classif(X_ohe, y, random_state=42)

mi_df = pd.DataFrame([mi], columns=X_ohe.columns, index=["Promotion_Eligible"])

plt.figure(figsize=(12, 3.5))
sns.heatmap(mi_df, annot=True, fmt=".3f", cmap="Blues")
plt.title("Mutual Information Heatmap (Feature â†’ Promotion_Eligible)")
plt.xticks(rotation=45, ha="right")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""## Multivariate Analysis

### Numerical Data Correlation

#### Label Encoding 'Current_Position_Level'
"""

def label_encode_position(df):
  position_map = {
      "Junior": 0,
      "Mid": 1,
      "Senior": 2,
      "Lead": 3
  }

  df["Position_Encoded"] = df["Current_Position_Level"].map(position_map)

  return df

df = label_encode_position(df)

"""#### Heatmap"""

num_cols = ['Age', 'Years_at_Company', 'Performance_Score', 'Leadership_Score','Training_Hours', 'Projects_Handled',
            'Peer_Review_Score', 'Position_Encoded', 'Promotion_Eligible']

fig, (ax1) = plt.subplots(1, 1, figsize=(11, 5))
# sns.heatmap(df[num_cols].corr(), cmap='Blues', annot=True, fmt='.2f')
sns.heatmap(df[num_cols].corr(method="spearman"), cmap='Blues', annot=True, fmt='.2f')

"""#### Conclusion

From the *Heatmap* above, it is found that there are little linear/monotonic relationship across all columns.

### Categorical Data Correlation

#### Mutual Information

##### 'Age' Mutual Information Analysis
"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Age"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""###### Conclusion

The low Mutual Information score across all columns suggest nearly all columns does not tell us information regarding 'Age'. However, with knowledge of the industry, we can assume that a Junior is unlikely to be 55 and Lead is unlikely to be 22. Therefore, we will be using 'Current_Position_Level' to impute missing/invalid 'Age' values.

##### 'Years_at_Company' Mutual Information Analysis
"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Years_at_Company"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""###### Conclusion

The low Mutual Information score across all columns suggest nearly all columns does not tell us information regarding 'Years_at_Company'. To force imputing values with any of these columns might impose personal biases (e.g not all seniors might have good KPI scores)

##### 'Performance_Score' Mutual Information Analysis
"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Years_at_Company",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Performance_Score"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""##### 'Leadership_Score' Mutual Information Analysis"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Years_at_Company",
                "Performance_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Leadership_Score"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""##### 'Training_Hours' Mutual Information Analysis"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Years_at_Company",
                "Leadership_Score", "Performance_Score", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Training_Hours"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""##### 'Projects_Handled' Mutual Information Analysis"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Years_at_Company",
                "Leadership_Score", "Performance_Score", "Training_Hours",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Projects_Handled"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""##### 'Peer_Review_Score' Mutual Information Analysis"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Years_at_Company",
                "Leadership_Score", "Performance_Score", "Projects_Handled",
                "Training_Hours", "Current_Position_Level", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Peer_Review_Score"]

mi = mutual_info_regression(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""##### 'Current_Position_Level' Mutual Information Analysis"""

cols = ["Age", "Years_at_Company", "Performance_Score",
                "Leadership_Score", "Training_Hours", "Projects_Handled",
                "Peer_Review_Score", "Current_Position_Level", "Position_Encoded", "Promotion_Eligible"]

df_mi = df[cols].dropna()

X = pd.get_dummies(
    df_mi[["Age", "Years_at_Company",
                "Leadership_Score", "Performance_Score", "Projects_Handled",
                "Training_Hours", "Peer_Review_Score", "Promotion_Eligible"]],
    drop_first=True
)
y = df_mi["Current_Position_Level"]

mi = mutual_info_classif(X, y, random_state=42)

pd.Series(mi, index=X.columns).sort_values(ascending=False)

"""#### Visualization"""

continuous_vars = [
    "Age", "Years_at_Company", "Performance_Score",
    "Leadership_Score", "Training_Hours",
    "Projects_Handled", "Peer_Review_Score"
]

categorical_vars = [
    "Current_Position_Level",
    "Promotion_Eligible"
]

all_vars = continuous_vars + categorical_vars

mi_table = pd.DataFrame("-", index=all_vars, columns=all_vars)

for target in all_vars:

    predictors = [v for v in all_vars if v != target]
    df_mi = df[predictors + [target]].dropna()

    X = pd.get_dummies(df_mi[predictors], drop_first=True)
    y = df_mi[target]

    # choose MI function
    if target in categorical_vars:
        mi = mutual_info_classif(X, y, random_state=42)
    else:
        mi = mutual_info_regression(X, y, random_state=42)

    mi_series = pd.Series(mi, index=X.columns)

    # aggregate dummy variables back to original feature names
    for col in predictors:
        matching = mi_series.filter(like=col)
        if len(matching) > 0:
            mi_table.loc[target, col] = matching.mean()

mi_table

"""Conclusion

The joint Mutual Information analysis indicates that while most variables exhibit weak individual dependency with each other, meaningful shared information emerges when structural variables are considered together. In particular, Age and Current_Position_Level show moderate dependency with Years_at_Company, reflecting career progression constraints rather than performance-driven effects

# Data Cleaning

## Finding Invalid/Missing Values
"""

def validate_data(df: pd.DataFrame) -> list[str]:
    errors = []

    # Check for missing values
    if df.isnull().any().any():
        null_cols = df.columns[df.isnull().any()].tolist()
        errors.append(
            f"Missing values found in columns: {', '.join(null_cols)}"
        )

    # Invalid numeric values
    numeric_cols = df.select_dtypes(include="number").columns
    if (df[numeric_cols] < 0).any().any():
        errors.append("Negative values found in numeric columns.")

    return errors

"""## Processing Invalid Rows & Columns

### Detecting Outlier Rows

#### Detecting Invalid 'Age'
"""

# Fixing 'Age' being too high or too low by rewriting it into N/A, which will be updated again in 'Filling Null/Invalid 'Age'' section
def detect_limit_age(df):
  MIN_AGE = 18
  MAX_AGE = 65

  invalid_age_mask = (df["Age"] < MIN_AGE) | (df["Age"] > MAX_AGE)
  df.loc[invalid_age_mask, "Age"] = pd.NA

  return df

# Fixing improbable 'Age' value when compared to 'Years_at_Company'
def detect_limit_age_tenure(df):
  MIN_HIRE_AGE = 18

  # detect rows where tenure implies impossible hire age
  invalid_tenure_mask = (
      df["Age"].notna() &
      df["Years_at_Company"].notna() &
      ((df["Age"] - df["Years_at_Company"]) < MIN_HIRE_AGE)
  )

  # invalidate Years_at_Company in those rows
  df.loc[invalid_tenure_mask, "Age"] = pd.NA

  return df

"""#### Detecting Invalid 'Years_at_Company'"""

# Fixing 'Years_at_Company' being too high or too low by rewriting it into N/A, which will be updated again in 'Filling Null/Invalid 'Age'' section
def detect_limit_tenure(df):
  MIN_YEARS = 0
  MAX_YEARS = 47

  invalid_years_mask = (df["Years_at_Company"] < MIN_YEARS) | (df["Years_at_Company"] > MAX_YEARS)
  df.loc[invalid_years_mask, "Years_at_Company"] = pd.NA

  return df

"""#### Detecting Invalid 'Training_Hours'"""

# Deleting negative and anomalously high 'Training_Hours'
def detect_limit_training_hours(df):
  Q1 = df["Training_Hours"].quantile(0.25)
  Q3 = df["Training_Hours"].quantile(0.75)
  IQR = Q3 - Q1

  upper = Q3 + 1.5 * IQR

  df["Training_Hours"] = df["Training_Hours"].clip(lower=0, upper=upper)

  return df

"""### Setting Outlier Columns as Null"""

df = detect_limit_age(df)
df = detect_limit_age_tenure(df)
df = detect_limit_tenure(df)
df = detect_limit_training_hours(df)

"""### Deleting Damaged Rows"""

def delete_null_rows(df):
  df = df[df.isna().sum(axis=1) < 3].copy()

  return df

df = delete_null_rows(df)

# rows with 3 or more null values
rows_3plus_null = df[df.isna().sum(axis=1) >= 3]

rows_3plus_null

"""## Fixing Columns"""

def fix_col(df, col_name):
  df[col_name] = df[col_name].fillna(
      df[col_name].median()
  )

  return df

"""## Fixing 'Age'"""

df.groupby('Current_Position_Level')['Age'].describe()

"""### Filling Null/Invalid 'Age'"""

def fix_age(df):
  # Fill null 'Age' with the median value in their respective 'Current_Position_Level'
  median_age_by_level = (
      df.groupby("Current_Position_Level")["Age"]
        .median()
  )

  df["Age"] = df["Age"].fillna(
      df["Current_Position_Level"].map(median_age_by_level)
  )

  # After filling out Null/Invalid 'Age', the difference between 'Age' and 'Years_at_Company' might be less than 18, in that case
  # set 'Age' to be 'Years_at_Company' + 18
  invalid_rows = df[df['Age'] - df['Years_at_Company'] < 18]
  df.loc[invalid_rows.index, 'Age'] = df.loc[invalid_rows.index, 'Years_at_Company'] + 18

  # If 'Current_Position_Level' is null, then use median to fill out this column
  df["Age"] = df["Age"].fillna(
      df["Age"].median()
  )

  # Once 'Age' has been filled, make sure that it does not exceed 65
  df.loc[df['Age'] > 65, 'Age'] = 65

  return df

df = fix_age(df)

df[df["Age"].isna()]

"""## Fixing 'Years_at_Company'"""

df.groupby('Current_Position_Level')['Years_at_Company'].describe()

"""### Filling Null/Invalid 'Years_at_Company'"""

df = fix_col(df, "Years_at_Company")

df[df["Years_at_Company"].isna()]

"""## Fixing 'Performance_Score'

### Filling Null 'Performance_Score'
"""

df = fix_col(df, "Performance_Score")

df[df["Performance_Score"].isna()]

"""## Fixing 'Leadership_Score'

### Filling Null 'Leadership_Score'
"""

df = fix_col(df, "Leadership_Score")

df[df["Leadership_Score"].isna()]

"""## Fixing 'Training_Hours'

### Filling Null/Invalid 'Training_Hours'
"""

df = fix_col(df, "Training_Hours")

df[df["Training_Hours"].isna()]

"""## Fixing 'Projects_Handled'

### Filling Null 'Projects_Handled'
"""

df = fix_col(df, "Projects_Handled")

df[df["Projects_Handled"].isna()]

"""## Fixing 'Peer_Review_Score'

### Filling Null 'Peer_Review_Score'
"""

df = fix_col(df, "Peer_Review_Score")

df[df["Peer_Review_Score"].isna()]

"""## Fixing 'Current_Position_Level'

Since the distribution of this columns is mostly equal across all the categories, and the one column ('Age') that has a high MI score does not have meaningful difference for its median and mean.

It is decided to fill null values as 'Unknown'.
"""

df.groupby('Current_Position_Level')['Age'].describe()

"""### Filling Null 'Current_Position_Level'"""

def fix_col_level(df):
  df["Current_Position_Level"] = df["Current_Position_Level"].fillna("Unknown")

  return df

df = fix_col_level(df)

df.describe(include='object')

"""# Feature Engineering"""

def feat_engineer(df):
  df["Avg_Score"] = df[["Performance_Score","Leadership_Score","Peer_Review_Score"]].mean(axis=1)
  df["Projects_per_Year"] = df["Projects_Handled"] / (df["Years_at_Company"] + 1)

  return df

df = feat_engineer(df)

"""# Feature Normalization"""

def feat_normal(df):
  df['Age_log'] = np.log1p(df['Age'])
  df['Years_at_Company_log'] = np.log1p(df['Years_at_Company'])
  df['Performance_Score_log'] = np.log1p(df['Performance_Score'])
  df['Leadership_Score_log'] = np.log1p(df['Leadership_Score'])
  df['Training_Hours_log'] = np.log1p(df['Training_Hours'])
  df['Projects_Handled_log'] = np.log1p(df['Projects_Handled'])
  df['Peer_Review_Score_log'] = np.log1p(df['Peer_Review_Score'])

  return df

df = feat_normal(df)

"""# One-Hot Encoding"""

def feat_encode(df):
  df = pd.get_dummies(df, columns=["Current_Position_Level"], drop_first=False)

  return df

df = feat_encode(df)

"""# Feature Scaling"""

def feat_scaling(df):
  df['Age_scaled'] = StandardScaler().fit_transform(df[['Age_log']])
  df['Years_at_Company_scaled'] = StandardScaler().fit_transform(df[['Years_at_Company_log']])
  df['Performance_Score_scaled'] = StandardScaler().fit_transform(df[['Performance_Score_log']])
  df['Leadership_Score_scaled'] = StandardScaler().fit_transform(df[['Leadership_Score_log']])
  df['Training_Hours_scaled'] = StandardScaler().fit_transform(df[['Training_Hours_log']])
  df['Projects_Handled_scaled'] = StandardScaler().fit_transform(df[['Projects_Handled_log']])
  df['Peer_Review_Score_scaled'] = StandardScaler().fit_transform(df[['Peer_Review_Score_log']])

  return df

df = feat_scaling(df)

"""# Feature Selection"""

def feat_select (df, col_list):
  df = df.drop(columns=col_list)

  return df

df.info()

df = feat_select(df, ['Employee_ID',  'Promotion_Eligible', 'Age_log', 'Years_at_Company_log', 'Performance_Score_log', 'Leadership_Score_log', 'Training_Hours_log',
                      'Projects_Handled_log', 'Peer_Review_Score_log', 'Position_Encoded', 'Avg_Score'])

"""# File Output"""

df.to_csv("Rakamin Bootcamp - Dataset - Promotion Dataset (Cleaned).csv", index=False)